% include the figures path relative to the master file
\graphicspath{ {./content/method/figures/} }

\section{Experimental Setup}\label{sec:exp}

The experimental set-up is summarized in Table~\ref{tab:survey-tab}.
Where the most relevant works in Sect.\,\ref{sec:review} are formulated as the as the 5-steps standard classification procedure described in Fig.\,\ref{fig:ML-scheme}.

\input{content/method/Survey-Table.tex}

\subsection{Implementation details}\label{sec:exp:implementation}
The experiments, described in this work, are publicly available at~\cite{rethinopaty20016apr-repoICPR} allowing for further comparisons and improvements.
Each method from Table~\ref{tab:survey-tab} has been implemented, using the library \emph{protoclass}~\cite{protoclass2016apr-repoICPR} to perform the image processing and \gls{ml} tasks.
Furthermore, each method has been evaluated in a common framework a stated in Sect.\,\ref{}.

{\color{red}Each methodology implementation can be seen as a plug-in to experiment in \cite{rethinopaty20016apr-repoICPR}, while references to stand-alone implementation of these methodologies can be found in Table~\ref{tab:survey-tab}.
All the repositories are publicly available and provided with tests to ensure that our implementation agrees with the results reported by the original works.}

Note that Liu~\emph{et~al.} train the algorithm at the B-scan level, and \gls{seri} dataset provides \gls{gt} at volume level only.
Thus, two strategies have been explored to solve this issue:
(i) similarly to Srinivasan~\emph{et~al.}, at training stage, all B-scans are considered as abnormal for a DME volume and at testing stage, a majority vote rule is applied to whether label a volume as abnormal or not;
(ii) similarly to Alsaih~\emph{et~al.}, an approach using BoW is used.
% The strategy to the best classification performance will be reported in Table~\ref{tab:summary_results}.
From the methods reviewed in Sect.\,\ref{sec:review}, we decline to implement Albarrak~\emph{et~al.} and Anantrasirichai~\textit{et~al.}.
The former does not provide sufficient implementation details to replicate their results~\cite{albarrak2013age};
while, the latter, uses a descriptor based on the layer thickness which require a layer segmentation stage using a generic segmentation algorithm and further user validation~\cite{anantrasirichai2013svm}.


\subsection{Evaluation}\label{sec:exp:evaluation}
All the experiments are evaluated in terms of \gls{se} and \gls{sp} (see Fig.\,\ref{fig:evaluation}) using the \gls{ltpocv} strategy, in line with \cite{Lemaintre2015miccaiOCT}.
Therefore, at each cross-validation iteration, a \gls{dme} and normal volumes are kept for testing, while the remaining volumes are used as training.
The \gls{se} evaluates the performance of the classifier with respect to the positive class, while the \gls{sp} evaluates its performance with respect to negative class.

\input{./content/method/figures/evaluation_corolary_def.tex}
\begin{figure}

  \def\myRadius{.65cm}
  \def\vennSpace{(0,0) rectangle (2.6cm,1.6cm)}
  \def\predictedCircle{(.8cm,.8cm) circle (\myRadius)}
  \def\actualCircle{(1.8cm,.8cm) circle (\myRadius)}
  \def\myLabelRadius{.450cm}

  \subfigure[][Confusion matrix with truly and falsely positive samples detected (TP, FP) in the first row, from left to right and the falsely and truly negative samples detected (FN, TN) in the second row, from left to right.]{
    \label{fig:evaluation:confusion_matrix}
    \begin{tikzpicture}[scale=0.5]
      \node at (0,0){
        \begin{tabular}{
            >{\centering}m{1em} >{\centering}m{1em} >{\centering}m{1in} >{\centering\arraybackslash}m{1in}}
          % c>{\centering}m{2em}ccc}
          & & \multicolumn{2}{c}{ Actual Class }\\
          & & A+ & A- \\
          % \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\usebox \centering Predicted Class}}}& P+ &  \tikz{\tp} & \tikz{\fp} \\
          \multirow{3}{*}{\rotatebox[origin=c]{90}{Predicted Class}}& P+ &  \tikz{\tp} & \tikz{\fp} \\
          & P- & \tikz{\fn} & \tikz{\tn}
        \end{tabular}
      };
    \end{tikzpicture}
  }\\
  \centering
  \subfigure[][\gls{se} and \gls{sp} evaluation, corresponding to the ratio of the doted area over the blue area.]{
    \label{fig:evaluation:roc_axis}
    \begin{tikzpicture}[scale=0.5]
      \def\seEquation{$SE = \frac{TP}{TP+FN}$}
      \def\spEquation{$SP = \frac{TN}{TN+FP}$}
      \node[label={[]below:\seEquation}](se){\tikz{\se}};
      \node[right=5pt of se, label={[]below:\spEquation}]{\tikz{\sp}};
      % \node[label={[]right:\seEquation}](se){\tikz{\se}};
      % \node[below=5pt of se, label={[]right:\spEquation}]{\tikz{\sp}};
    \end{tikzpicture}
  }

  \caption{Evaluation metrics:
    \protect\subref{fig:evaluation:confusion_matrix} confusion matrix,
    \protect\subref{fig:evaluation:roc_axis} \gls{se} - \gls{sp}
  }
  \label{fig:evaluation}
\end{figure}

The use of \gls{ltpocv} implies that at each round, a pair \gls{dme}-normal volume is selected for testing while the remaining volumes are used for training.
Subsequently, no \gls{se} or \gls{sp} variance can be reported.
However, \gls{ltpocv} strategy has been adopted despite this limitation due to the reduced size of the dataset.

% \subsection{Management of data depending terms}
% \deleted[id=sik]{
%   Be aware that when computing the GMM~\cite{repo_des}, or the dictionary
%   \cite{repo_lem}, only the training data for the current fold is used.
%   Therefore such modules are recomputed at each fold.
% }
% \deleted[id=sik]{
%   Other parameter tuning such the case of XXXX and YYYYY are also carried out using only ZZZZ
% }
